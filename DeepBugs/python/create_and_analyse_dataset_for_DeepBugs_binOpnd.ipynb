{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create Dataset for DeepBugs wrong binary operand\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from typing import List, Dict, Union\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import json\n",
    "import codecs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "benchmark_dir = '../benchmarks'\n",
    "\n",
    "# Path to the dataset files where bugs were seeded\n",
    "# data_path = os.path.join(benchmark_dir,'binOps_data.pkl')\n",
    "data_path = os.path.join(benchmark_dir,'binOps_data.pkl')\n",
    "data_dir = os.path.join(benchmark_dir, 'binOps_data')\n",
    "\n",
    "# Path to the files after seeding the bugs\n",
    "wrong_binary_operand_path = os.path.join(benchmark_dir,'binOps_wrong_operand.pkl')\n",
    "wrong_binary_operand_dir = os.path.join(benchmark_dir, 'binOps_wrong_operand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_json_file(json_file_path)->Dict:\n",
    "    try:\n",
    "        obj_text = codecs.open(json_file_path, 'r', encoding='utf-8').read()\n",
    "        r = json.loads(obj_text)\n",
    "        return r\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            \"Please provide a correct file p. Eg. ./results/validated-conflicts.json\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        # Empty JSON file most likely due to abrupt killing of the process while writing\n",
    "        # print (e)\n",
    "        return {}\n",
    "\n",
    "def read_dataset_given_files(extracted_data_files: List) -> pd.DataFrame:\n",
    "    d = []\n",
    "    with Pool(cpu_count()) as p:\n",
    "        with tqdm(total=len(extracted_data_files)) as pbar:\n",
    "            pbar.set_description_str(\n",
    "                desc=\"Reading dataset from files\", refresh=False)\n",
    "            for i, each_vars in enumerate(\n",
    "                    p.imap_unordered(read_json_file, extracted_data_files, 20)):\n",
    "                pbar.update()\n",
    "                d.extend(each_vars)\n",
    "            p.close()\n",
    "            p.join()\n",
    "    extracted_dataset = pd.DataFrame(d)\n",
    "    return extracted_dataset\n",
    "\n",
    "def file_path_to_dataset(dataset_file_path, dir_path):\n",
    "    if not Path(dataset_file_path).is_file():\n",
    "        file_paths = list(Path(dir_path).rglob('*.json'))\n",
    "        print(f\"Number of files={len(file_paths)}\")\n",
    "        dataset = read_dataset_given_files(extracted_data_files=file_paths)\n",
    "        print(f\"Saving {dataset_file_path}\")\n",
    "        dataset.to_pickle(dataset_file_path,'gzip')\n",
    "    else:\n",
    "        print(f'Reading from {dataset_file_path}')\n",
    "        dataset = pd.read_pickle(dataset_file_path,'gzip')\n",
    "    print(f\"Dataset contains {len(dataset)} examples\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wrongbinOpndSeeded = file_path_to_dataset(dataset_file_path=wrong_binary_operand_path, dir_path=wrong_binary_operand_dir)\n",
    "wrongbinOpndSeeded.rename(columns={\"src\": \"file\"}, inplace=True)\n",
    "binopData = file_path_to_dataset(dataset_file_path=data_path, dir_path=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "binopData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the seeded bugs to extract the location of seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_binary_operand_loc_path = wrong_binary_operand_path.replace('.pkl','_withloc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_content(file_path: Path) -> Union[List, Dict]:\n",
    "    content = []\n",
    "    try:\n",
    "        with codecs.open(str(file_path), 'r', encoding='utf-8') as f:\n",
    "            c = f.read()\n",
    "            content = json.loads(c)\n",
    "    except FileNotFoundError:\n",
    "        print(f'Not found {file_path} ')\n",
    "        pass\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return content\n",
    "\n",
    "def get_location_of_seeded(row):\n",
    "    analysed_location=row['file']\n",
    "    bug_seeding_metadata = read_file_content('../'+analysed_location.split(' :')[0] + 'on')\n",
    "    file_name = bug_seeding_metadata['file_name_where_intended']\n",
    "    line = bug_seeding_metadata['target_line_range']['line'].split('-')\n",
    "    \n",
    "    # Represents the range of the source and not of the seeded bug\n",
    "    rng_data = bug_seeding_metadata['target_line_range']['range'] \n",
    "    line = ' - '.join(line)\n",
    "    location_seeded_bug = file_name + ' : ' + line\n",
    "    \n",
    "    return location_seeded_bug, rng_data\n",
    "    # no extra characters were added/deleted to seed the bug \n",
    "    #if row['range'] == rng_seeded: \n",
    "    #     return location_seeded_bug, rng_seeded\n",
    "    #else:\n",
    "    #    return location_seeded_bug, rng_seeded\n",
    "\n",
    "if not Path(wrong_binary_operand_loc_path).is_file():\n",
    "    rows_iter = (row for _, row in wrongbinOpndSeeded.iterrows())\n",
    "    locations = []\n",
    "    ranges_source = []\n",
    "    with Pool(cpu_count()) as p:\n",
    "        with tqdm(total=len(wrongbinOpndSeeded)) as pbar:\n",
    "            pbar.set_description_str(\n",
    "                    desc=\"Getting locations\", refresh=False)\n",
    "            for i, rt in enumerate(p.map(get_location_of_seeded, rows_iter, 10)):\n",
    "                loc, ranges_src = rt\n",
    "                locations.append(loc)\n",
    "                ranges_source.append(ranges_src)\n",
    "                pbar.update()\n",
    "            p.close()\n",
    "            p.join()\n",
    "    wrongbinOpndSeeded['src'] = locations\n",
    "    wrongbinOpndSeeded['range'] = ranges_source\n",
    "    print(f'Saving to {wrong_binary_operand_loc_path}')\n",
    "    wrongbinOpndSeeded.to_pickle(wrong_binary_operand_loc_path, 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongbinOpndSeeded_loc=pd.read_pickle(wrong_binary_operand_loc_path, 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wrongbinOpndSeeded_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create the dataset for DeepBugs\n",
    "\n",
    "Map the location from seeded bugs to the original files.\n",
    "First remove the duplicates from both datasets and then merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_out_path = os.path.join(benchmark_dir,'dataset_for_deepbugs_binOpnd4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binopData['range_str'] = binopData['range'].apply(lambda x: str(x))\n",
    "wrongbinOpndSeeded_loc['range_str'] = wrongbinOpndSeeded_loc['range'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrongbinOpndSeeded_loc = wrongbinOpndSeeded_loc.drop(columns=['file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the corresponding row from targer files where bug was seeded \n",
    "\n",
    "For each seeded bug, find the exact line and range from the extracted data.\n",
    "Unfortunately, we can't always find the exact location because of formatting issues.\n",
    "We have tried all possible ways to format but for some cases, there is no way to have an exact formatting.\n",
    "This offsets the line and range in one of the file and it is not possible to pinpoint the exact location where\n",
    "the bug was seeded. As a result, we loose a lot of seeded bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correspondig_buggy_row(row):\n",
    "    same_locs = binopData[binopData['src']==row['src']]\n",
    "    for _, data_row in same_locs.iterrows():\n",
    "        if data_row['range_str'] == row['range_str']:\n",
    "            if (str(row['left'])+str(row['right'])) != (str(data_row['left'])+str(data_row['right'])) and row['op']==data_row['op']:\n",
    "                return data_row.name\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corresponding_row_file_path = os.path.join(benchmark_dir, 'binOpnd4_correct_rows.json')\n",
    "cor_row = []\n",
    "\n",
    "if not Path(corresponding_row_file_path).is_file():\n",
    "    rows_iter = [row for _, row in wrongbinOpndSeeded_loc.iterrows()]\n",
    "\n",
    "    with Pool(cpu_count()//3) as p:\n",
    "        with tqdm(total=len(rows_iter)) as pbar:\n",
    "            pbar.set_description_str(desc=\"Extracting location\", refresh=False)\n",
    "            for _, rw_num in enumerate(p.map(get_correspondig_buggy_row, rows_iter)):\n",
    "                cor_row.append(rw_num)\n",
    "                pbar.update()\n",
    "            p.close()\n",
    "            p.join()\n",
    "\n",
    "    with open(corresponding_row_file_path, 'w+') as f:\n",
    "        json.dump(cor_row, f)\n",
    "else:\n",
    "    with open(corresponding_row_file_path, 'r') as f:\n",
    "        cor_row = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongbinOpndSeeded_loc['corrsp_row'] = cor_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongbinOpndSeeded_loc = wrongbinOpndSeeded_loc[wrongbinOpndSeeded_loc['corrsp_row']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongbinOpndSeeded_loc = wrongbinOpndSeeded_loc.drop(columns=['range_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongbinOpndSeeded_loc['probability_that_incorrect'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_buggy_rows = []\n",
    "rows_iter = [row for _, row in wrongbinOpndSeeded_loc.iterrows()]\n",
    "for rw in tqdm(rows_iter):\n",
    "    dr = rw['corrsp_row']\n",
    "    r = binopData.iloc[dr].to_dict()\n",
    "    r['probability_that_incorrect'] = 0\n",
    "    r['file'] = rw['file']\n",
    "    non_buggy_rows.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongbinOpndSeeded_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dataset = pd.read_pickle(merged_out_path,'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_buggy = pd.DataFrame(non_buggy_rows)\n",
    "non_buggy = non_buggy.drop(columns=['range_str'])\n",
    "non_buggy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buggy = wrongbinOpndSeeded_loc.drop(columns= ['corrsp_row'])\n",
    "merged = pd.concat([non_buggy, buggy], ignore_index=True)\n",
    "print(f\"Size of dataset={len(merged)}\")\n",
    "print(f\"Writing to {merged_out_path}\")\n",
    "merged.to_pickle(merged_out_path, 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now create training and validation datasets\n",
    "\n",
    "Deepbugs expects the training and validation datasets to be '.json' files. First split the merged dataset and then create the required datasets.\n",
    "\n",
    "The dataset format looks like the following\n",
    "```js\n",
    "[\n",
    "    [\n",
    "        { // non-buggy\n",
    "          \"left\": \"ID:g\",\n",
    "          \"right\": \"LIT:67\",\n",
    "          \"op\": \">\",\n",
    "          \"leftType\": \"unknown\",\n",
    "          \"rightType\": \"number\",\n",
    "          \"parent\": \"IfStatement\",\n",
    "          \"grandParent\": \"BlockStatement\",\n",
    "          \"src\": \"benchmarks/data/data/1.js : 6 - 6\",\n",
    "          \"probability_that_incorrect\": 0\n",
    "        },\n",
    "        { // buggy\n",
    "          \"left\": \"ID:g\",\n",
    "          \"right\": \"LIT:67\",\n",
    "          \"op\": \">=\",\n",
    "          \"leftType\": \"unknown\",\n",
    "          \"rightType\": \"number\",\n",
    "          \"parent\": \"IfStatement\",\n",
    "          \"grandParent\": \"BlockStatement\",\n",
    "          \"src\": \"benchmarks/js_benchmark_seeded_bugs/1_SEMSEED_MUTATED_1.js : 6 - 6\",\n",
    "          \"probability_that_incorrect\": 1\n",
    "        }\n",
    "    ],\n",
    "    [  ]\n",
    " ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.read_pickle(merged_out_path,'gzip')\n",
    "buggy = merged[merged['probability_that_incorrect']==1]\n",
    "non_buggy = merged[merged['probability_that_incorrect']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Buggy={len(buggy)}, Non-buggy={len(non_buggy)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "buggy.iloc[12220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "non_buggy.iloc[12220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buggy_iter = [row for _, row in buggy.iterrows()]\n",
    "nbuggy_iter = [row for _, row in non_buggy.iterrows()]\n",
    "\n",
    "dataset = []\n",
    "for bg, nbg in tqdm(zip(buggy_iter, nbuggy_iter), desc='creating dataset', total=len(buggy_iter)):\n",
    "    dataset.append([bg.to_dict(), nbg.to_dict()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset[12220]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and validation dataset\n",
    "\n",
    "# We only use training change pattern during seeding, so no need to filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(content, out_file):\n",
    "    with open(out_file, 'w+') as f:\n",
    "        print(f'Writing to {f.name}')\n",
    "        json.dump(content, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Size of dataset={len(dataset)}')\n",
    "write_json(dataset,os.path.join(benchmark_dir, 'full_dataset_wrong_binopnd4.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame([b_o_n for td in dataset for b_o_n in td])\n",
    "dataset_df.to_pickle(merged_out_path, 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Select only those seeded bugs that are present in the training patterns\n",
    "\n",
    "Use only the 'training' change patterns as mentioned in the paper. First read all\n",
    "change patterns and then split it 80-20.\n",
    "\n",
    "Next, select only those that conform our change pattern selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_only_idf_lit_containing_patterns(all_changes):\n",
    "    \"\"\"\n",
    "    It is possible that every bug-fix pattern can not be used to seed bugs.\n",
    "    We filter some of them here. For example:\n",
    "        * we may filter very long change patterns (although we do it once while aggregating data from MongoDB)\n",
    "        * we may select only those chage patterns that has atleast 'N' frequency\n",
    "    \"\"\"\n",
    "    filtered_change_patterns = []\n",
    "    for t in all_changes:\n",
    "        # If the change pattern contains at-least one Identifier/Literal, we use that.\n",
    "        # Else the change pattern is discarded\n",
    "        if 'Idf_' in ' '.join(t['fix']) or 'Idf_' in ' '.join(t['buggy']) or 'Lit_' in ' '.join(\n",
    "                t['fix']) or 'Lit_' in ' '.join(t['buggy']):\n",
    "            filtered_change_patterns.append(t)\n",
    "\n",
    "    return filtered_change_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_change_patterns = read_json_file(os.path.join(benchmark_dir, 'bug_seeding_patterns_for_semantic_seeding.json'))\n",
    "all_change_patterns = get_only_idf_lit_containing_patterns(all_change_patterns)\n",
    "print(f'Found {len(all_change_patterns)} patterns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "l_len = len(all_change_patterns)*80 // 100\n",
    "tr_patterns, val_patterns = all_change_patterns[:l_len], all_change_patterns[l_len:]\n",
    "print(f'Number of training patterns = {len(tr_patterns)}, Number of validation patterns = {len(val_patterns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not use the validation patterns here. Rather we will use them as examples of real bugs that *DeepBugs*\n",
    "will try to find.\n",
    "\n",
    "So now select only those seeded bugs that has been seeded using *url* present in the training patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset where DeepBugs will seed artificial bugs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous dataset includes both the correct and the seeded bugs. Now, we discard the seeded bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "complete_dataset_no_seeded_included=[]\n",
    "for _, rw in tqdm(binopData.iterrows(), total=len(binopData)):\n",
    "    row = rw.to_dict()\n",
    "    complete_dataset_no_seeded_included.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "write_json(complete_dataset_no_seeded_included,os.path.join(benchmark_dir, 'full_dataset_wrong_binOpnd_no_seeded_included.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create a combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "combined_dataset = list(complete_dataset_no_seeded_included)\n",
    "for d in tqdm(dataset, desc='Creating combined dataset'):\n",
    "    combined_dataset.append(d[0])\n",
    "    # combined_dataset.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Length of combined = {len(combined_dataset)}')\n",
    "write_json(combined_dataset,os.path.join(benchmark_dir, 'full_dataset_wrong_binOpnd_combined.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"No of training, validation examples {len(tr_df_no_seeded_included)},{len(vl_df_no_seeded_included)} and full dataset {len(dataset_no_seeded_included)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample = binopData.sample(10)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample['src'].apply(lambda x: x.split(':')[0].lstrip().rstrip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
