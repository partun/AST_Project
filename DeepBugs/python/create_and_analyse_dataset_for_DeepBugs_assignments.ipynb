{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset for DeepBugs for wrong assignment bugs\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from typing import List, Dict, Union\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import json\n",
    "import codecs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "benchmark_dir = '../benchmarks'\n",
    "\n",
    "# Path to the dataset files where bugs were seeded\n",
    "data_path = os.path.join(benchmark_dir,'assignments_data.pkl')\n",
    "data_dir = os.path.join(benchmark_dir, 'assignments_data')\n",
    "\n",
    "# Path to the files after seeding the bugs\n",
    "wrong_assignment_seeded_path = os.path.join(benchmark_dir,'assignments_wrong.pkl')\n",
    "wrong_assignment_seeded_dir = os.path.join(benchmark_dir, 'assignments_wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_json_file(json_file_path)->Dict:\n",
    "    try:\n",
    "        obj_text = codecs.open(json_file_path, 'r', encoding='utf-8').read()\n",
    "        return json.loads(obj_text)\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            f\"{json_file_path} not found, provide a correct file path\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        # Empty JSON file most likely due to abrupt killing of the process while writing\n",
    "        # print (e)\n",
    "        return {}\n",
    "\n",
    "def read_dataset_given_files(extracted_data_files: List) -> pd.DataFrame:\n",
    "    d = []\n",
    "    with Pool(cpu_count()) as p:\n",
    "        with tqdm(total=len(extracted_data_files)) as pbar:\n",
    "            pbar.set_description_str(\n",
    "                desc=\"Reading dataset from files\", refresh=False)\n",
    "            for i, each_vars in enumerate(\n",
    "                    p.imap_unordered(read_json_file, extracted_data_files, 20)):\n",
    "                pbar.update()\n",
    "                d.extend(each_vars)\n",
    "            p.close()\n",
    "            p.join()\n",
    "    extracted_dataset = pd.DataFrame(d)\n",
    "    return extracted_dataset\n",
    "\n",
    "def file_path_to_dataset(dataset_file_path, dir_path):\n",
    "    if not Path(dataset_file_path).is_file():\n",
    "        file_paths = list(Path(dir_path).rglob('*.json'))\n",
    "        # Debug\n",
    "        print(f\"Number of files={len(file_paths)}\")\n",
    "        dataset = read_dataset_given_files(extracted_data_files=file_paths)\n",
    "        print(f\"Saving {dataset_file_path}\")\n",
    "        dataset.to_pickle(dataset_file_path,'gzip')\n",
    "    else:\n",
    "        print(f'Reading from {dataset_file_path}')\n",
    "        dataset = pd.read_pickle(dataset_file_path,'gzip')\n",
    "    print(f\"Dataset contains {len(dataset)} examples\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wrong_assignment_seeded = file_path_to_dataset(dataset_file_path=wrong_assignment_seeded_path, dir_path=wrong_assignment_seeded_dir)\n",
    "wrong_assignment_seeded.rename(columns={\"src\": \"file\"}, inplace=True)\n",
    "assignments_data = file_path_to_dataset(dataset_file_path=data_path, dir_path=data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wrong_assignment_seeded_loc_path = wrong_assignment_seeded_path.replace('.pkl','_withloc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file_content(file_path: Path) -> Union[List, Dict]:\n",
    "    content = []\n",
    "    try:\n",
    "        with codecs.open(str(file_path), 'r', encoding='utf-8') as f:\n",
    "            c = f.read()\n",
    "            content = json.loads(c)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return content\n",
    "\n",
    "def get_location_of_seeded(row):\n",
    "    analysed_location=row['file']\n",
    "    bug_seeding_metadata = read_file_content('../'+analysed_location.split(' :')[0] + 'on')\n",
    "    file_name = bug_seeding_metadata['file_name_where_intended']\n",
    "    line = bug_seeding_metadata['target_line_range']['line'].split('-')\n",
    "\n",
    "    # Represents the range of the source and not of the seeded bug\n",
    "    rng_data = bug_seeding_metadata['target_line_range']['range']\n",
    "    line = ' - '.join(line)\n",
    "    location_seeded_bug = file_name + ' : ' + line\n",
    "\n",
    "    return location_seeded_bug, rng_data\n",
    "    # no extra characters were added/deleted to seed the bug\n",
    "    #if row['range'] == rng_seeded:\n",
    "    #     return location_seeded_bug, rng_seeded\n",
    "    #else:\n",
    "    #    return location_seeded_bug, rng_seeded\n",
    "\n",
    "if not Path(wrong_assignment_seeded_loc_path).is_file():\n",
    "    rows_iter = (row for _, row in wrong_assignment_seeded.iterrows())\n",
    "    locations = []\n",
    "    ranges_source = []\n",
    "    with Pool(cpu_count()) as p:\n",
    "        with tqdm(total=len(wrong_assignment_seeded)) as pbar:\n",
    "            pbar.set_description_str(\n",
    "                    desc=\"Getting locations\", refresh=False)\n",
    "            for i, rt in enumerate(p.map(get_location_of_seeded, rows_iter, 10)):\n",
    "                loc, ranges_src = rt\n",
    "                locations.append(loc)\n",
    "                ranges_source.append(ranges_src)\n",
    "                pbar.update()\n",
    "            p.close()\n",
    "            p.join()\n",
    "    wrong_assignment_seeded['src'] = locations\n",
    "    wrong_assignment_seeded['range'] = ranges_source\n",
    "    print(f'Saving to {wrong_assignment_seeded_loc_path}')\n",
    "    wrong_assignment_seeded.to_pickle(wrong_assignment_seeded_loc_path, 'gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wrong_assignment_seeded_loc=pd.read_pickle(wrong_assignment_seeded_loc_path, 'gzip')\n",
    "#print(get_location_of_seeded(wrong_assignment_seeded_loc.iloc[0]))\n",
    "print(f'Size is {len(wrong_assignment_seeded_loc)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(wrong_assignment_seeded_loc.iloc[10,6])\n",
    "print(wrong_assignment_seeded_loc.iloc[10,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create the dataset for DeepBugs\n",
    "Map the location from seeded bugs to the original files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged_out_path = os.path.join(benchmark_dir,'dataset_for_deepbugs_wrong_assignments.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assignments_data['range_str'] = assignments_data['range'].apply(lambda x: str(x))\n",
    "wrong_assignment_seeded_loc['range_str'] = wrong_assignment_seeded_loc['range'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_correspondig_buggy_row(row):\n",
    "    same_locs = assignments_data[assignments_data['src']==row['src']]\n",
    "    for _, data_row in same_locs.iterrows():\n",
    "        if data_row['range_str'] == row['range_str']:\n",
    "            if (str(row['lhs'])+str(row['rhs'])) != (str(data_row['lhs'])+str(data_row['rhs'])):\n",
    "                return data_row.name\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corresponding_row_file_path = os.path.join(benchmark_dir, 'wrong_assignment_correct_rows.json')\n",
    "cor_row = []\n",
    "\n",
    "if not Path(corresponding_row_file_path).is_file():\n",
    "    rows_iter = [row for _, row in wrong_assignment_seeded_loc.iterrows()]\n",
    "\n",
    "    with Pool(cpu_count()//2) as p:\n",
    "        with tqdm(total=len(rows_iter)) as pbar:\n",
    "            pbar.set_description_str(desc=\"Extracting location\", refresh=False)\n",
    "            for _, rw_num in enumerate(p.map(get_correspondig_buggy_row, rows_iter)):\n",
    "                cor_row.append(rw_num)\n",
    "                pbar.update()\n",
    "            p.close()\n",
    "            p.join()\n",
    "\n",
    "    with open(corresponding_row_file_path, 'w+') as f:\n",
    "        json.dump(cor_row, f)\n",
    "else:\n",
    "    with open(corresponding_row_file_path, 'r') as f:\n",
    "        print(f'Reading {f.name}')\n",
    "        cor_row = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wrong_assignment_seeded_loc['corrsp_row'] = cor_row\n",
    "wrong_assignment_seeded_loc = wrong_assignment_seeded_loc[wrong_assignment_seeded_loc['corrsp_row']!=-1]\n",
    "len(wrong_assignment_seeded_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wrong_assignment_seeded_loc = wrong_assignment_seeded_loc.drop(columns=['range_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wrong_assignment_seeded_loc['probability_that_incorrect'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "non_buggy_rows = []\n",
    "rows_iter = [row for _, row in wrong_assignment_seeded_loc.iterrows()]\n",
    "for rw in tqdm(rows_iter):\n",
    "    dr = rw['corrsp_row']\n",
    "    # Get the corresponding row from 'data'\n",
    "    r = assignments_data.iloc[dr].to_dict()\n",
    "    r['probability_that_incorrect'] = 0\n",
    "    r['file'] = rw['file']\n",
    "    non_buggy_rows.append(r)\n",
    "non_buggy = pd.DataFrame(non_buggy_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "buggy = wrong_assignment_seeded_loc.drop(columns= ['corrsp_row'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged = pd.concat([non_buggy, buggy], ignore_index=True)\n",
    "print(f\"Size of dataset={len(merged)}\")\n",
    "print(f\"Writing to {merged_out_path}\")\n",
    "merged.to_pickle(merged_out_path, 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged = pd.read_pickle(merged_out_path,'gzip')\n",
    "buggy = merged[merged['probability_that_incorrect']==1]\n",
    "non_buggy = merged[merged['probability_that_incorrect']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(non_buggy.iloc[10])\n",
    "print(buggy.iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "buggy_iter = [row for _, row in buggy.iterrows()]\n",
    "nbuggy_iter = [row for _, row in non_buggy.iterrows()]\n",
    "\n",
    "dataset = []\n",
    "for bg, nbg in tqdm(zip(buggy_iter, nbuggy_iter), desc='creating dataset', total=len(buggy_iter)):\n",
    "    dataset.append([bg.to_dict(), nbg.to_dict()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame([b_o_n for td in dataset for b_o_n in td])\n",
    "dataset_df = dataset_df.drop(columns=['range_str'])\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_df['seeding_url']=dataset_df['file'].apply(lambda x: read_json_file('../'+x.split(':')[0].lstrip().rstrip()+'on')['seeding_pattern_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_df.to_pickle(merged_out_path, 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def write_json(content, out_file):\n",
    "    with open(out_file, 'w+') as f:\n",
    "        print(f'Writing to {f.name}')\n",
    "        json.dump(content, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "write_json(dataset,os.path.join(benchmark_dir, 'full_dataset_wrong_assignment.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Select only those seeded bugs that are present in the training patterns\n",
    "\n",
    "Use only the 'training' change patterns as mentioned in the paper. First read all\n",
    "change patterns and then split it 80-20.\n",
    "\n",
    "Next, select only those that conform our change pattern selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_change_patterns = read_json_file(os.path.join(benchmark_dir, 'bug_seeding_patterns_for_semantic_seeding.json'))\n",
    "print(f'Found {len(all_change_patterns)} patterns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "l_len = len(all_change_patterns)*80 // 100\n",
    "tr_patterns, val_patterns = all_change_patterns[:l_len], all_change_patterns[l_len:]\n",
    "print(f'Number of training patterns = {len(tr_patterns)}, Number of validation patterns = {len(val_patterns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We do not use the validation patterns here. Rather we will use them as examples of real bugs that *DeepBugs*\n",
    "will try to find.\n",
    "\n",
    "So now select only those seeded bugs that has been seeded using *url* present in the training patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tr_patterns = pd.DataFrame(tr_patterns)\n",
    "tr_urls = set(tr_patterns['url'])\n",
    "print(f'There exists {len(tr_urls)} unique urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged = pd.read_pickle(merged_out_path,'gzip')\n",
    "buggy = merged[merged['probability_that_incorrect']==1]\n",
    "non_buggy = merged[merged['probability_that_incorrect']==0]\n",
    "\n",
    "buggy_iter = [row for _, row in buggy.iterrows()]\n",
    "nbuggy_iter = [row for _, row in non_buggy.iterrows()]\n",
    "\n",
    "dataset = []\n",
    "for bg, nbg in tqdm(zip(buggy_iter, nbuggy_iter), desc='creating dataset only from training', total=len(buggy_iter)):\n",
    "    if bg['seeding_url'] in tr_urls and nbg['seeding_url'] in tr_urls:\n",
    "        dataset.append([bg.to_dict(), nbg.to_dict()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now write the filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'There contains {len(dataset)*2} examples after filtering. The original contained {len(dataset_df)} examples')\n",
    "write_json(dataset,os.path.join(benchmark_dir, 'full_dataset_wrong_assignment.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For artificial seeding during DeepBugs training, we use the complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "complete_dataset_no_seeded_included=[]\n",
    "for _, rw in tqdm(assignments_data.iterrows(), total=len(assignments_data)):\n",
    "    row = rw.to_dict()\n",
    "    complete_dataset_no_seeded_included.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "write_json(complete_dataset_no_seeded_included,os.path.join(benchmark_dir, 'full_dataset_wrong_assignment_no_seeded_included.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Estimate the number of files where bugs were not seeded\n",
    "This is only needed if I need to re-run bug seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "files_containing_assignments = set(assignments_data['src'].apply(lambda x: x.split(':')[0].lstrip().rstrip()))\n",
    "files_already_seeded = set(dataset_df['src'].apply(lambda x: x.split(':')[0].lstrip().rstrip()))\n",
    "files_not_seeded = list(files_containing_assignments - files_already_seeded)\n",
    "with open(os.path.join(benchmark_dir,'files_containing_assignments_not_seeded.json'),'w+') as f:\n",
    "    print(f'Writing to {f.name}')\n",
    "    json.dump(files_not_seeded,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Files containing assignments {len(files_containing_assignments)}')\n",
    "print(f'Files already seeded {len(files_already_seeded)}')\n",
    "print(f'Files not yet seeded {len(files_not_seeded)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
