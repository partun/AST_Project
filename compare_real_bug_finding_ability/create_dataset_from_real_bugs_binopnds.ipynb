{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create dataset from real bugs extracted from GitHub\n",
    "Here we go through the binary operations extracted from the commits of real bugs in GitHub. The goal is to creat\n",
    "a dataset in a format expected by DeepBugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import codecs\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm.notebook import trange, tqdm\n",
    "benchmarks_dir = '../benchmarks'\n",
    "\n",
    "real_bugs_dataset_file_path  = os.path.join(benchmarks_dir, 'binops_real_bugs.pkl')\n",
    "real_bugs_dataset_dir  = os.path.join(benchmarks_dir, 'binops_real_bugs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_json_file(json_file_path)->Dict:\n",
    "    try:\n",
    "        obj_text = codecs.open(json_file_path, 'r', encoding='utf-8').read()\n",
    "        return json.loads(obj_text)\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            \"Please provide a correct file p. Eg. ./results/validated-conflicts.json\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        # Empty JSON file most likely due to abrupt killing of the process while writing\n",
    "        # print (e)\n",
    "        return {}\n",
    "\n",
    "def read_dataset_given_files(extracted_data_files: List) -> pd.DataFrame:\n",
    "    d = []\n",
    "    with Pool(cpu_count()) as p:\n",
    "        with tqdm(total=len(extracted_data_files)) as pbar:\n",
    "            pbar.set_description_str(\n",
    "                desc=\"Reading dataset from files\", refresh=False)\n",
    "            for i, each_vars in enumerate(\n",
    "                    p.imap_unordered(read_json_file, extracted_data_files, 20)):\n",
    "                pbar.update()\n",
    "                d.extend(each_vars)\n",
    "            p.close()\n",
    "            p.join()\n",
    "    extracted_dataset = pd.DataFrame(d)\n",
    "    return extracted_dataset\n",
    "\n",
    "def file_path_to_dataset(dataset_file_path, dir_path):\n",
    "    if not Path(dataset_file_path).is_file():\n",
    "        file_paths = list(Path(dir_path).rglob('*.json'))\n",
    "        print(f\"Number of files={len(file_paths)}\")\n",
    "        dataset = read_dataset_given_files(extracted_data_files=file_paths)\n",
    "        print(f\"Saving {dataset_file_path}\")\n",
    "        dataset.to_pickle(dataset_file_path,'gzip')\n",
    "    else:\n",
    "        print(f'Reading from {dataset_file_path}')\n",
    "        dataset = pd.read_pickle(dataset_file_path,'gzip')\n",
    "    print(f\"Dataset contains {len(dataset)} examples\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_file_loc(row):\n",
    "    d = row.to_dict()\n",
    "    if 'benchmarks/real_bugs_github/buggy_' in d['src']:\n",
    "        file_name = d['src'].replace('benchmarks/real_bugs_github/buggy_','')\n",
    "    else:\n",
    "        file_name = d['src'].replace('benchmarks/real_bugs_github/correct_','')\n",
    "    range = str(d['range'])\n",
    "    return file_name+'_'+range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = file_path_to_dataset(dataset_file_path=real_bugs_dataset_file_path, dir_path=real_bugs_dataset_dir)\n",
    "row_iter = [row for _, row in dataset.iterrows()]\n",
    "locations = []\n",
    "for row in tqdm(row_iter):\n",
    "    loc = get_file_loc(row)\n",
    "    locations.append(loc)\n",
    "dataset['filename_loc'] = locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "correct_dataset = dataset[dataset['src'].apply(lambda x: 'correct_' in x)]\n",
    "buggy_dataset = dataset[dataset['src'].apply(lambda x: 'buggy_' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged = correct_dataset.merge(buggy_dataset,left_on='filename_loc', right_on='filename_loc', suffixes=['_CORRECT','_BUGGY'])\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_buggy_non_buggy_data(row):\n",
    "    d = row.to_dict()\n",
    "    correct = {k.replace('_CORRECT',''):v for k, v in d.items() if '_CORRECT' in k}\n",
    "    correct['probability_that_incorrect'] = 0\n",
    "    buggy = {k.replace('_BUGGY',''):v for k, v in d.items() if '_BUGGY' in k}\n",
    "    buggy['probability_that_incorrect'] = 1\n",
    "    if (correct['left'] != buggy['left'] or correct['right'] != buggy['right'] ) and correct['op'] == buggy['op'] :\n",
    "        return [correct, buggy]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "correct_bin_ops = []\n",
    "buggy_bin_ops = []\n",
    "x_y_pair_given = []\n",
    "for _,row in tqdm(list(merged.iterrows()), desc='Get lines'):\n",
    "    r = get_buggy_non_buggy_data(row)\n",
    "    if len(r):\n",
    "        correct_bin_ops.append(r[0])\n",
    "        buggy_bin_ops.append(r[1])\n",
    "        x_y_pair_given.append(r)\n",
    "print(f'Number of buggy/correct binOps extracted are {len(correct_bin_ops)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(x_y_pair_given))\n",
    "filtered_x_y_pair = []\n",
    "for pr in x_y_pair_given:\n",
    "    if pr[0]['parent'] =='AwaitExpression' or 'AwaitExpression' == pr[0]['grandParent']:\n",
    "        continue\n",
    "    if pr[1]['parent'] =='AwaitExpression'or 'AwaitExpression' == pr[1]['grandParent']:\n",
    "        continue\n",
    "    filtered_x_y_pair.append(pr)\n",
    "x_y_pair_given = filtered_x_y_pair\n",
    "print(len(x_y_pair_given))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give the buggy lines as input to a trained model in DeepBugs and check how many are actually classified as buggy.\n",
    "Then we confirm them with the correct extracted binops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def write_json(content, out_file):\n",
    "    with open(out_file, 'w+') as f:\n",
    "        json.dump(content, f)\n",
    "\n",
    "# write_json(correct_bin_ops, os.path.join(benchmarks_dir, 'correct_real_binops.json'))\n",
    "# write_json(buggy_bin_ops, os.path.join(benchmarks_dir, 'buggy_real_binops.json'))\n",
    "write_json(x_y_pair_given, os.path.join(benchmarks_dir, 'correct_buggy_real_binops.json'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
